"""Run Bayesian analysis of data from a paired-comparison experiment.
This script should be used as a template,
to be copied and modified for any particular experiment.

*** Usage, four main steps, see also explicit template example below

*1: Create a PairedCompFrame instance to define experiment and select input data.

*2: Load a set of experimental paired-comparison data

*3: Learn Bayesian model corresponding to observed data
using either Thurstone Case V or Bradley-Terry-Luce probabilistic choice model.

*4: Display results and save figures and tables to a directory tree

*** Version history:

* Version 2.0:
2021-09-13, new function pc_display.display; user interface to select display variants
"""

from pathlib import Path
import pickle
import logging

from PairedCompCalc import pc_logging, __version__
from PairedCompCalc.pc_data import PairedCompFrame, PairedCompDataSet
from PairedCompCalc.pc_model import PairedCompResultSet
from PairedCompCalc.pc_display import display

# -----------------------------------
# from PairedCompCalc.pc_model import Bradley
# model_class = Bradley
from PairedCompCalc.pc_model import Thurstone
model_class = Thurstone
# = selected class of probabilistic choice model for the analysis

# ---------------------- location of input and output data:
work_path = Path.home() / 'Documents' / 'PairedComp_sim'  # data generated by run_sim.py
# work_path = Path.home() / 'Documents' / 'My_PairedComp_project'  # or whatever...

data_path = work_path / 'data'
# = directory to be searched for read-only input data files

result_path = work_path / 'result'
# = top directory for all result files,
# with sub-directories created as needed.
# NOTE: Existing result files in this directory are OVER-WRITTEN WITHOUT WARNING,
# but different analysis runs may add new result files to the same existing directory.

assert result_path != data_path, 'Result directory must be different from input data directory'

model_result_file = 'pc_result.pkl'
# = name of file with saved PairedCompResultSet instance, if used

display_file = 'pc_displays.pkl'
# = name of file with saved PairedCompDisplaySet instance, if used

log_file = 'run_pc_log.txt'
# = name of log file

result_path.mkdir(parents=True, exist_ok=True)
pc_logging.setup(result_path, log_file)
logging.info(f'*** Running PairedCompCalc version {__version__}')

# ---------------------- Define experimental structure:

# NOTE: all string values are CASE-sensitive,
# e.g., objects 'A' and 'a' are DIFFERENT,

# pcf = PairedCompFrame(attributes=['Speech Clarity', 'Pleasantness', 'Preference'],
#                       objects=['Program0', 'Program1', 'Program2'], # may be taken from input data
#                       objects_alias=['A', 'B', 'C', 'D'],  # to hide real object names in displays
#                       forced_choice=False,
#                       difference_grades=['Equal', 'Slightly Better', 'Better', 'Much Better'],
#                       test_factors={'Background': ['Quiet', 'Noisy']}
#                       )

# Test example for data generated by run_sim.py:
pcf = PairedCompFrame(attributes=['SimQ'],
                      # objects=[f'HA{i}' for i in range(3)],  # accepting all from input data file
                      forced_choice=False,
                      difference_grades=['Equal', 'Slightly Better', 'Better', 'Much Better'],
                      test_factors={'Stim': ['speech', 'music'], 'SNR': ['Quiet', 'High']}
                      )


# ---------------------- Main analysis work:

logging.info(f'Analysing paired-comparison data in {data_path}')

# Using all xlsx data files in data_path and its sub-directories, for example:
# Using data generated and saved as xlsx files by run_sim.py:

ds = PairedCompDataSet.load(pcf=pcf, dir=data_path, fmt='xlsx',
                            groups=['Group0'],  # group directories under data_path
                            # sheets=[f'Subject{i}' for i in range(10)],  # included xlsx sheets
                            top_row=2,  # skip header row
                            subject='sheet',  # subject ID stored as sheet name
                            attribute='A',  # Attribute in column 'A'
                            pair=('B', 'C'),  # etc.
                            difference='D',
                            # choice='E',  # column with selected object
                            # no_choice=['None', ''],  # labels indicating no response, or no preference
                            # grade='F',  # category label of difference grade
                            Stim='G',
                            SNR='H',
                            )
# NOTE: run_sim.py saved one file for each subject, but
# a single xlsx file can also include results for several subjects and attributes.

# Responses may be stored
# EITHER as difference = a signed integer (preferred)
# OR as choice = chosen object in the presented pair,
#   AND grade = one category from pcf.difference_grades
# run_sim.py saved responses in both ways, just to illustrate the encoding.

# In this example from run_sim, the category labels for both test factors
# are stored in columns 'G' and 'H'.
# However, test-factor labels may also be implicitly defined as substrings in path names,
#   if not specified in a column in the data sheet.
# *** See pc_file_xlsx for more details about the xlsx input file format

# Optionally, using the same data stored as 'json' files by run_sim.py:
# ds = PairedCompDataSet.load(pcf, data_path, fmt='json', groups=['Group0'])

logging.info(f'Learning Results with model {model_class}. Might take a few minutes...')
pc_result = PairedCompResultSet.learn(ds, rv_class=model_class)

# ------------------------------- Optionally, dump learned result set:
with (result_path / model_result_file).open('wb') as f:
    pickle.dump(pc_result, f)

# The dumped pickle file may be re-loaded later to save learning time,
# in case different display formats are desired:
# ------------------------------- use pre-saved result set:
# with (result_path / model_result_file).open('rb') as f:
#     pc_result = pickle.load(f)

# ------------------------------- Generate result displays:

pc_display_set = display(pc_result)
# = default display set, showing estimated results for
# (1) random individual in the population from which participants were recruited,
# (2) the population mean
# (3) overview of participating subject groups

# OR, using different display formats, for example:

# pc_display_set = display(pc_result,
#                          population_individual=True,  # random individual in the population
#                          population_mean=False,  # skip population mean
#                          subject_group=True,  # overview of subject groups
#                          subject_individual=True,  # include detailed individual results
#                          percentiles=[2.5, 25., 50., 75., 97.5],  # displayed percentiles
#                          credibility_limit=0.8,  # joint probability of differences
#                          show_intervals=False,  # no response thresholds in plots
#                          object='Hearing Aid',  # label of tested object in display headings
#                          figure_format='pdf',  # or 'eps', or 'jpg', or 'png'
#                          table_format='latex',  # or 'tab' for tab-separated text
#                          )

# *** See pc_display.FMT and pc_display_format.FMT for all available format parameters
#   and their default settings.

# ***** Edit display plots or tables here, if needed *****
# Each display element can be accessed and modified by user, before saving,

# -------------- Optional Likelihood Ratio significance test:
# NOTE: Training the NULL model takes same time as the actual model.
# Likelihood-Ratio p-value may NOT be accurate.
# Safer to use the Bayesian predictive probabilities shown in pc_display_set tables.
# Uncomment the following code to get the test results:

# logging.info(f'Running Likelihood Ratio Significance Test. ' +
#              'Null Hypothesis: Population mean quality parameters all equal.')
# logging.info('*** NOTE: Likelihood-Ratio p-values are approximate ***')
# logging.info(f'Learning Null-hypothesis model with {model_class}')
#
# pc_result_null = PairedCompResultSet.learn(ds, rv_class=model_class, null_quality=True)
# pc_display_set.likelihood_ratio_test(pc_result_null, pc_result)


# ------------------------------- save all result displays:
pc_display_set.save(result_path)

# Optionally, save the display set as a single editable object:
# if display_file is not None:
#     with (result_path / display_file).open('wb') as f:
#         pickle.dump(pc_display_set, f)
# *** can be loaded again, elements edited manually, and re-saved

logging.info(f'All results saved in {result_path} and sub-dirs.')
logging.shutdown()
