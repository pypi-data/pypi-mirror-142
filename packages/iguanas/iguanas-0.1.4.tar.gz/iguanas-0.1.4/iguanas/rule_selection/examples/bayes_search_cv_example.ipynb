{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0d32be5",
   "metadata": {},
   "source": [
    "# Bayes Search CV Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be487a30",
   "metadata": {},
   "source": [
    "The `BayesSearchCV` class is used to search for the set of hyperparameters that produce the best decision engine performance for a given Iguanas Pipeline, whilst also reducing the likelihood of overfitting.\n",
    "\n",
    "The process is as follows:\n",
    "\n",
    "* Generate k-fold stratified cross validation datasets. \n",
    "* For each of the training and validation datasets:\n",
    "    * Fit the pipeline on the training set using a set of parameters chosen by the Bayesian Optimiser from a given set of ranges.\n",
    "    * Apply the pipeline to the validation set to return a prediction.\n",
    "    * Use the provided `scorer` to calculate the score of the prediction.\n",
    "* Return the parameter set which generated the highest mean overall score across the validation datasets.\n",
    "\n",
    "In this example, we'll consider the following workflow:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3fc3247",
   "metadata": {},
   "source": [
    "<center><img src=\"images/workflow_example.png\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e00e669",
   "metadata": {},
   "source": [
    "We'll use the `BayesSearchCV` class to optimise the hyperparameters of the steps in this workflow, **ensuring that we acquire the maximum F1 score for our decision engine.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0aba9dc",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b438fcc3",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "250846dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from iguanas.rule_generation import RuleGeneratorDT\n",
    "from iguanas.rule_selection import SimpleFilter, CorrelatedFilter, BayesSearchCV\n",
    "from iguanas.metrics import FScore, JaccardSimilarity\n",
    "from iguanas.rbs import RBSOptimiser, RBSPipeline\n",
    "from iguanas.correlation_reduction import AgglomerativeClusteringReducer\n",
    "from iguanas.pipeline import LinearPipeline\n",
    "from iguanas.pipeline.class_accessor import ClassAccessor\n",
    "from iguanas.space import UniformFloat, UniformInteger, Choice\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from category_encoders.one_hot import OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fce42ec",
   "metadata": {},
   "source": [
    "## Read in data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0492642",
   "metadata": {},
   "source": [
    "Let's read in the famous Titanic data set and split it into training and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3cba0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../../examples/dummy_data/titanic.csv', index_col='PassengerId')\n",
    "target_col = 'Survived'\n",
    "cols_to_drop = ['Name', 'Ticket', 'Cabin']\n",
    "X = df.drop([target_col] + cols_to_drop, axis=1)\n",
    "y = df[target_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7bdbd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.33,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fba4d34",
   "metadata": {},
   "source": [
    "## Data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f01bec7",
   "metadata": {},
   "source": [
    "Let's apply the following simple steps to process the data:\n",
    "\n",
    "* One hot encode categorical variables (accounting for nulls)\n",
    "* Impute numeric features with -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8697c40a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jlaidler/venvs/iguanas_dev/lib/python3.8/site-packages/category_encoders/utils.py:21: FutureWarning: is_categorical is deprecated and will be removed in a future version.  Use is_categorical_dtype instead\n",
      "  elif pd.api.types.is_categorical(cols):\n"
     ]
    }
   ],
   "source": [
    "# OHE\n",
    "encoder = OneHotEncoder(\n",
    "    use_cat_names=True\n",
    ")\n",
    "X_train = encoder.fit_transform(X_train)\n",
    "X_test = encoder.transform(X_test)\n",
    "\n",
    "# Impute\n",
    "X_train.fillna(-1, inplace=True)\n",
    "X_test.fillna(-1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225e38dd",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35bc7364",
   "metadata": {},
   "source": [
    "## Set up pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc67c9cc",
   "metadata": {},
   "source": [
    "Before we can apply the `BayesSearchCV` class, we need to set up our pipeline. Let's first instantiate the classes we'll be using the our pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11021686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimisation metric\n",
    "f1 = FScore(beta=1)\n",
    "\n",
    "# Rule generation\n",
    "generator = RuleGeneratorDT(\n",
    "    metric=f1.fit,\n",
    "    n_total_conditions=4,\n",
    "    tree_ensemble=RandomForestClassifier(\n",
    "        n_estimators=10,\n",
    "        random_state=0\n",
    "    )\n",
    ")\n",
    "\n",
    "# Rule filter (performance-based)\n",
    "simple_filterer = SimpleFilter(\n",
    "    threshold=0.1, \n",
    "    operator='>=', \n",
    "    metric=f1.fit\n",
    ")\n",
    "\n",
    "# Rule filter (correlation-based)\n",
    "js = JaccardSimilarity()\n",
    "corr_filterer = CorrelatedFilter(\n",
    "    correlation_reduction_class=AgglomerativeClusteringReducer(\n",
    "        threshold=0.9, \n",
    "        strategy='top_down', \n",
    "        similarity_function=js.fit, \n",
    "        metric=f1.fit\n",
    "    )\n",
    ")\n",
    "\n",
    "# Decision engine (to be optimised)\n",
    "rbs_pipeline = RBSPipeline(\n",
    "    config=[],\n",
    "    final_decision=0\n",
    ")\n",
    "\n",
    "# Decision engine optimiser\n",
    "rbs_optimiser = RBSOptimiser(\n",
    "    pipeline=rbs_pipeline,\n",
    "    metric=f1.fit, \n",
    "    pos_pred_rules=ClassAccessor(\n",
    "        class_tag='corr_filterer', \n",
    "        class_attribute='rules_to_keep'\n",
    "    ),\n",
    "    rules=ClassAccessor(\n",
    "        class_tag='generator',\n",
    "        class_attribute='rules'\n",
    "    ),\n",
    "    n_iter=10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297ea112",
   "metadata": {},
   "source": [
    "**Note:** The arguments passed to the `pos_pred_rules` and `rules` parameters in the `RBSOptimiser` class are `ClassAccessor` objects. This object extracts the specified attribute from the given class in the pipeline. This allows users to pass attributes from earlier steps in the pipeline as parameters of later steps in the pipeline.\n",
    "\n",
    "In this example, the names of the rules that are present after the `corr_filterer` step are passed to the `pos_pred_rules` parameter of the `RBSOptimiser` class - this is so the `RBSOptimiser` knows which rules predict positive cases (which, one might argue, doesn't need to be specified in this example, as we only have one type of rule set. However, when you have a set of rules - some that predict positive cases and some that predict negative cases - you must specify which rules predict what case, using the `pos_pred_rules` and `neg_pred_rules` parameters).\n",
    "\n",
    "Also, the `rules` attribute created in the `generator` step are passed to the `rules` parameter of the `RBSOptimiser` class. This is so the rules remaining after the decision engine optimisation can be easily extracted from the trained pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5446d91",
   "metadata": {},
   "source": [
    "Now we can create the steps of our pipeline. Each step should be a tuple of two elements:\n",
    "\n",
    "1. The first element should be a string which refers to the step.\n",
    "2. The second element should be the instantiated class which is run as part of the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5bb0c41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = [\n",
    "    ('generator', generator),\n",
    "    ('simple_filterer', simple_filterer),\n",
    "    ('corr_filterer', corr_filterer),\n",
    "    ('rbs_optimiser', rbs_optimiser)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace623b4",
   "metadata": {},
   "source": [
    "Finally, we can instantiate our pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4efb672",
   "metadata": {},
   "outputs": [],
   "source": [
    "lp = LinearPipeline(steps=steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101b040d",
   "metadata": {},
   "source": [
    "## Define the search space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed60794",
   "metadata": {},
   "source": [
    "Now we need to define the search space for each of the relevant parameters of our pipeline. To do this, we create a dictionary, where each key corresponds to the tag used for the relevant pipeline step. Each value should be a dictionary of the parameters (keys) and their search spaces (values). Search spaces should be defined using the classes in the `iguanas.space` module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "218a3ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_spaces = {\n",
    "    'generator': {\n",
    "        'n_total_conditions': UniformInteger(1, 5),\n",
    "        'target_feat_corr_types': Choice([\n",
    "            'Infer',\n",
    "            None\n",
    "        ])\n",
    "    },\n",
    "    'simple_filterer': {\n",
    "        'threshold': UniformFloat(0, 1),\n",
    "    },\n",
    "    'corr_filterer': {\n",
    "        'correlation_reduction_class': Choice([\n",
    "            AgglomerativeClusteringReducer(\n",
    "                threshold=0.5, \n",
    "                strategy='top_down', \n",
    "                similarity_function=js.fit, \n",
    "                metric=f1.fit\n",
    "            ),\n",
    "            AgglomerativeClusteringReducer(\n",
    "                threshold=0.9, \n",
    "                strategy='top_down', \n",
    "                similarity_function=js.fit, \n",
    "                metric=f1.fit\n",
    "            )\n",
    "        ])\n",
    "    },    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440b959e",
   "metadata": {},
   "source": [
    "Based on the search spaces above, we'll be optimising the following parameters across the following ranges:\n",
    "\n",
    "* **generator**\n",
    "    * `n_total_conditions`: Integers from 1 to 5\n",
    "    * `target_feat_corr_types`: Either 'Infer' or None.\n",
    "* **simple_filterer**\n",
    "    * `threshold`: Floats from 0 to 1\n",
    "* **corr_filterer**\n",
    "    * `correlation_reduction_class`: `AgglomerativeClusteringReducer` classes with either a `threshold` of 0.5 or 0.9."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63e6476",
   "metadata": {},
   "source": [
    "## Optimise the pipeline hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80181bf4",
   "metadata": {},
   "source": [
    "Now that we have our pipeline and search spaces defined, we can instantiate the `BayesSearchCV` class. We'll split our data into 3 cross-validation datasets and try 20 different parameter sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2060a0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = BayesSearchCV(\n",
    "    pipeline=lp, \n",
    "    search_spaces=search_spaces, \n",
    "    metric=f1.fit, \n",
    "    cv=3, \n",
    "    n_iter=20,\n",
    "    num_cores=3,\n",
    "    error_score=0,\n",
    "    verbose=1    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee3ff2a",
   "metadata": {},
   "source": [
    "Finally, we can run the `fit` method to optimise the hyperparameters of the pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af7f1e91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Optimising pipeline parameters ---\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:23<00:00,  1.16s/trial, best loss: -0.6965755602560381]\n",
      "--- Refitting on entire dataset with best pipeline ---\n"
     ]
    }
   ],
   "source": [
    "bs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2486b3a4",
   "metadata": {},
   "source": [
    "### Outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3ad83c",
   "metadata": {},
   "source": [
    "The `fit` method doesn't return anything. See the `Attributes` section in the class docstring for a description of each attribute generated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b22e783f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6965755602560381"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs.best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "137d6bc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'corr_filterer': {'correlation_reduction_class': AgglomerativeClusteringReducer(threshold=0.5, strategy=top_down, similarity_function=<bound method JaccardSimilarity.fit of JaccardSimilarity>, metric=<bound method FScore.fit of FScore with beta=1>, print_clustermap=False)},\n",
       " 'generator': {'n_total_conditions': 2.0, 'target_feat_corr_types': 'Infer'},\n",
       " 'simple_filterer': {'threshold': 0.4860473230215504}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5b4df48b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs.best_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "49e61e98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Params</th>\n",
       "      <th>corr_filterer__correlation_reduction_class</th>\n",
       "      <th>generator__n_total_conditions</th>\n",
       "      <th>generator__target_feat_corr_types</th>\n",
       "      <th>simple_filterer__threshold</th>\n",
       "      <th>FoldIdx</th>\n",
       "      <th>Scores</th>\n",
       "      <th>MeanScore</th>\n",
       "      <th>StdDevScore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'corr_filterer': {'correlation_reduction_clas...</td>\n",
       "      <td>AgglomerativeClusteringReducer(threshold=0.5, ...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Infer</td>\n",
       "      <td>0.486047</td>\n",
       "      <td>[0, 1, 2]</td>\n",
       "      <td>[0.7092198581560283, 0.6842105263157895, 0.696...</td>\n",
       "      <td>0.696576</td>\n",
       "      <td>0.010212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>{'corr_filterer': {'correlation_reduction_clas...</td>\n",
       "      <td>AgglomerativeClusteringReducer(threshold=0.5, ...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Infer</td>\n",
       "      <td>0.644417</td>\n",
       "      <td>[0, 1, 2]</td>\n",
       "      <td>[0.7092198581560283, 0.6842105263157895, 0.696...</td>\n",
       "      <td>0.696576</td>\n",
       "      <td>0.010212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>{'corr_filterer': {'correlation_reduction_clas...</td>\n",
       "      <td>AgglomerativeClusteringReducer(threshold=0.5, ...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>None</td>\n",
       "      <td>0.497632</td>\n",
       "      <td>[0, 1, 2]</td>\n",
       "      <td>[0.7092198581560283, 0.6615384615384615, 0.691...</td>\n",
       "      <td>0.687496</td>\n",
       "      <td>0.019695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>{'corr_filterer': {'correlation_reduction_clas...</td>\n",
       "      <td>AgglomerativeClusteringReducer(threshold=0.9, ...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Infer</td>\n",
       "      <td>0.388521</td>\n",
       "      <td>[0, 1, 2]</td>\n",
       "      <td>[0.6783625730994152, 0.6153846153846154, 0.682...</td>\n",
       "      <td>0.658794</td>\n",
       "      <td>0.030745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>{'corr_filterer': {'correlation_reduction_clas...</td>\n",
       "      <td>AgglomerativeClusteringReducer(threshold=0.5, ...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>None</td>\n",
       "      <td>0.304850</td>\n",
       "      <td>[0, 1, 2]</td>\n",
       "      <td>[0.6511627906976745, 0.5925925925925927, 0.694...</td>\n",
       "      <td>0.646224</td>\n",
       "      <td>0.041919</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Params  \\\n",
       "0   {'corr_filterer': {'correlation_reduction_clas...   \n",
       "14  {'corr_filterer': {'correlation_reduction_clas...   \n",
       "19  {'corr_filterer': {'correlation_reduction_clas...   \n",
       "13  {'corr_filterer': {'correlation_reduction_clas...   \n",
       "18  {'corr_filterer': {'correlation_reduction_clas...   \n",
       "\n",
       "           corr_filterer__correlation_reduction_class  \\\n",
       "0   AgglomerativeClusteringReducer(threshold=0.5, ...   \n",
       "14  AgglomerativeClusteringReducer(threshold=0.5, ...   \n",
       "19  AgglomerativeClusteringReducer(threshold=0.5, ...   \n",
       "13  AgglomerativeClusteringReducer(threshold=0.9, ...   \n",
       "18  AgglomerativeClusteringReducer(threshold=0.5, ...   \n",
       "\n",
       "    generator__n_total_conditions generator__target_feat_corr_types  \\\n",
       "0                             2.0                             Infer   \n",
       "14                            5.0                             Infer   \n",
       "19                            2.0                              None   \n",
       "13                            2.0                             Infer   \n",
       "18                            4.0                              None   \n",
       "\n",
       "    simple_filterer__threshold    FoldIdx  \\\n",
       "0                     0.486047  [0, 1, 2]   \n",
       "14                    0.644417  [0, 1, 2]   \n",
       "19                    0.497632  [0, 1, 2]   \n",
       "13                    0.388521  [0, 1, 2]   \n",
       "18                    0.304850  [0, 1, 2]   \n",
       "\n",
       "                                               Scores  MeanScore  StdDevScore  \n",
       "0   [0.7092198581560283, 0.6842105263157895, 0.696...   0.696576     0.010212  \n",
       "14  [0.7092198581560283, 0.6842105263157895, 0.696...   0.696576     0.010212  \n",
       "19  [0.7092198581560283, 0.6615384615384615, 0.691...   0.687496     0.019695  \n",
       "13  [0.6783625730994152, 0.6153846153846154, 0.682...   0.658794     0.030745  \n",
       "18  [0.6511627906976745, 0.5925925925925927, 0.694...   0.646224     0.041919  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs.cv_results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1b4ad8",
   "metadata": {},
   "source": [
    "To see the final optimised decision engine configuration and rule set, we first return the parameters of the trained pipeline (stored in the attribute `pipeline_`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2b18d2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_params = bs.pipeline_.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6fef44d",
   "metadata": {},
   "source": [
    "Then, to see the final optimised decision engine configuration, we filter to the `config` parameter of the `rbs_optimiser` step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4882ba88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, ['RGDT_Rule_20220301_18'])]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_config = pipeline_params['rbs_optimiser']['config']\n",
    "final_config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa67e52a",
   "metadata": {},
   "source": [
    "This shows us which rules should be used for the rejection step (decision `1`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936d9c3a",
   "metadata": {},
   "source": [
    "To see the logic of our final set of rules, we filter to the `rules` parameter of the `rbs_optimiser` step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0d4a2997",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_rules = bs.pipeline_.get_params()['rbs_optimiser']['rules']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd0c10b",
   "metadata": {},
   "source": [
    "Then extract the `rule_strings` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1d9a1caa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'RGDT_Rule_20220301_18': \"(X['Sex_male']==False)\"}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_rules.rule_strings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b795ebe8",
   "metadata": {},
   "source": [
    "## Apply the optimised pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5d86d2",
   "metadata": {},
   "source": [
    "We can apply our optimised pipeline to a new data set and make a prediction using the `predict` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4f448c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test = bs.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250f9302",
   "metadata": {},
   "source": [
    "### Outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23adbb1",
   "metadata": {},
   "source": [
    "The `predict` method returns the prediction generated by class in the final step of the pipeline - in this case, the `RBSOptimiser`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fe02d803",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PassengerId\n",
       "710    0\n",
       "440    0\n",
       "841    0\n",
       "721    1\n",
       "40     1\n",
       "      ..\n",
       "716    0\n",
       "526    0\n",
       "382    1\n",
       "141    1\n",
       "174    0\n",
       "Length: 295, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9491f459",
   "metadata": {},
   "source": [
    "We can now calculate the F1 score of our optimised pipeline using the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6e3fa768",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_opt = f1.fit(y_pred_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4e4e70",
   "metadata": {},
   "source": [
    "Comparing this to our original, unoptimised pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7a948cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "lp.fit(X_train, y_train, None)\n",
    "y_pred_test_init = lp.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a5d0a421",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_init = f1.fit(y_pred_test_init, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8de0cd7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score of original, unoptimised pipeline: 0.57\n",
      "F1 score of optimised pipeline: 0.74\n",
      "Percentage improvement in F1 score: 30.39%\n"
     ]
    }
   ],
   "source": [
    "print(f'F1 score of original, unoptimised pipeline: {round(f1_init, 2)}')\n",
    "print(f'F1 score of optimised pipeline: {round(f1_opt, 2)}')\n",
    "print(f'Percentage improvement in F1 score: {round(100*(f1_opt-f1_init)/f1_init, 2)}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f440a43f",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6a5a22224d030f6805b27da964f50b3905be89918ca593f843e32c3b2a80fa84"
  },
  "kernelspec": {
   "display_name": "iguanas_dev",
   "language": "python",
   "name": "iguanas_dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
