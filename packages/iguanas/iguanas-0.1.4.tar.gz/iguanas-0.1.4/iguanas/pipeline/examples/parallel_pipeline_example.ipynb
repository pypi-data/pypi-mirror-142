{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4314dd6f",
   "metadata": {},
   "source": [
    "# Parallel Pipeline Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3a33f2",
   "metadata": {},
   "source": [
    "A Parallel Pipeline is a set of steps which run independently - their outputs are then combined and returned. Each step should be an instantiated class with both `fit` and `transform` methods.\n",
    "\n",
    "The below diagram shows the high level structure of a Parallel Pipeline:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a66889",
   "metadata": {},
   "source": [
    "<center><img src=\"images/parallel_pipeline_structure.png\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557a086d",
   "metadata": {},
   "source": [
    "It is used to carry out multiple processes seperately and combine their outputs. One such use case is when we want to both:\n",
    "\n",
    "1. Generate a new rule set, and:\n",
    "2. Optimise an existing rule set, then:\n",
    "3. Use the combined rule sets to optimise a decision engine\n",
    "\n",
    "An example of this workflow is shown below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028c813f",
   "metadata": {},
   "source": [
    "<center><img src=\"images/parallel_pipeline_example.png\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5066b13",
   "metadata": {},
   "source": [
    "To create this workflow in Iguanas, the rule generation and rule optimisation steps are added to a Parallel Pipeline, so they run separately and their outputs are combined. This Parallel Pipeline is then added to a Linear Pipeline, along with the decision engine optimisation step, so that the output of the Parallel Pipeline is fed into the decision engine optimiser.\n",
    "\n",
    "**We'll see how this workflow can be generated in the following example.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be721b29",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ddc510f",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb7115b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from iguanas.rule_generation import RuleGeneratorDT\n",
    "from iguanas.rule_optimisation import BayesianOptimiser\n",
    "from iguanas.rule_selection import SimpleFilter, CorrelatedFilter\n",
    "from iguanas.metrics import FScore, Precision, JaccardSimilarity\n",
    "from iguanas.rbs import RBSOptimiser, RBSPipeline\n",
    "from iguanas.correlation_reduction import AgglomerativeClusteringReducer\n",
    "from iguanas.pipeline import LinearPipeline, ParallelPipeline, ClassAccessor\n",
    "from iguanas.rules import Rules\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from category_encoders.one_hot import OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6031c6",
   "metadata": {},
   "source": [
    "## Read in data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28354c2",
   "metadata": {},
   "source": [
    "Let's read in the famous Titanic data set and split it into training and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "938ebd72",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../../examples/dummy_data/titanic.csv', index_col='PassengerId')\n",
    "target_col = 'Survived'\n",
    "cols_to_drop = ['Name', 'Ticket', 'Cabin']\n",
    "X = df.drop([target_col] + cols_to_drop, axis=1)\n",
    "y = df[target_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb3ee24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_raw, X_test_raw, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.33,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0499265",
   "metadata": {},
   "source": [
    "## Data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae50c1c2",
   "metadata": {},
   "source": [
    "Let's apply the following simple steps to process the data:\n",
    "* One hot encode categorical variables (accounting for nulls)\n",
    "* Impute numeric features with -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fac7d3e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jlaidler/venvs/iguanas_dev/lib/python3.8/site-packages/category_encoders/utils.py:21: FutureWarning: is_categorical is deprecated and will be removed in a future version.  Use is_categorical_dtype instead\n",
      "  elif pd.api.types.is_categorical(cols):\n"
     ]
    }
   ],
   "source": [
    "# OHE\n",
    "encoder = OneHotEncoder(\n",
    "    use_cat_names=True\n",
    ")\n",
    "X_train = encoder.fit_transform(X_train_raw)\n",
    "X_test = encoder.transform(X_test_raw)\n",
    "\n",
    "# Impute\n",
    "X_train.fillna(-1, inplace=True)\n",
    "X_test.fillna(-1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264a6921",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d043f0",
   "metadata": {},
   "source": [
    "## Set up pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a914a91d",
   "metadata": {},
   "source": [
    "To create the worflow shown at the beginning of the notebook, let's first assume we have the following existing rule set (stored in the standard Iguanas string format):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91e0e386",
   "metadata": {},
   "outputs": [],
   "source": [
    "existing_rules = Rules(\n",
    "    rule_strings = {\n",
    "        'AgeRule': \"(X['Age']>20)|(X['Age'].isna())\",\n",
    "        'CombinedRule': \"(X['Pclass']<=2)&(X['Sex']=='female')\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0d6a7b",
   "metadata": {},
   "source": [
    "**Note:** these rules use the unprocessed data, as they contain conditions that either look for null values or compare to a given category. We'll need to use the unprocessed data when optimising these rules."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419ac6d2",
   "metadata": {},
   "source": [
    "To use in a rule optimiser, we need to convert the rules to the standard Iguanas lambda expression format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6b5448f",
   "metadata": {},
   "outputs": [],
   "source": [
    "existing_rule_lambdas = existing_rules.as_rule_lambdas(\n",
    "    as_numpy=False, \n",
    "    with_kwargs=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5df3d3",
   "metadata": {},
   "source": [
    "Now let's instantiate the classes used in our Parallel Pipeline - this will consist of a rule generation step and a rule optimisation step. We'll optimise both our rules and our decision engine to maximise the **F1 score**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23d51fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimisation metric\n",
    "f1 = FScore(beta=1)\n",
    "\n",
    "# Rule generation\n",
    "generator = RuleGeneratorDT(\n",
    "    metric=f1.fit,\n",
    "    n_total_conditions=3,\n",
    "    tree_ensemble=RandomForestClassifier(\n",
    "        n_estimators=2,\n",
    "        random_state=0\n",
    "    ),\n",
    "    target_feat_corr_types='Infer'\n",
    ")\n",
    "# Rule optimisation\n",
    "optimiser = BayesianOptimiser(\n",
    "    rule_lambdas=existing_rule_lambdas,\n",
    "    lambda_kwargs=existing_rules.lambda_kwargs,\n",
    "    metric=f1.fit,\n",
    "    n_iter=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f7fc59",
   "metadata": {},
   "source": [
    "Now we can create the steps of our Parallel Pipeline. Each step should be a tuple of two elements:\n",
    "\n",
    "1. The first element should be a string which refers to the step.\n",
    "2. The second element should be the instantiated class which runs at that step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2bb4844a",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = [\n",
    "    ('generator', generator),\n",
    "    ('optimiser', optimiser)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0705d55f",
   "metadata": {},
   "source": [
    "Then we can instantiate our Parallel Pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73ea0e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the ParallelPipeline\n",
    "pp = ParallelPipeline(\n",
    "    steps=steps,\n",
    "    verbose=1 # Set to 1 to see overall progress\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b115363",
   "metadata": {},
   "source": [
    "Now that we have our Parallel Pipeline defined, we can define our Linear Pipeline, the first step of which will be our Parallel Pipeline. This will run the rule generation and optimisation steps separately, combine their outputs, and feed it into the decision engine optimiser (our second step in the Linear Pipeline):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0fcfc577",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision engine (to be optimised)\n",
    "rbs_pipeline = RBSPipeline(\n",
    "    config=[], # Use an empty list here - the RBSOptimiser will create the config\n",
    "    final_decision=0\n",
    ")\n",
    "\n",
    "# Decision engine optimiser\n",
    "rbs_optimiser = RBSOptimiser(\n",
    "    pipeline=rbs_pipeline,\n",
    "    metric=f1.fit, \n",
    "    pos_pred_rules=ClassAccessor(\n",
    "        class_tag='pp', \n",
    "        class_attribute='rule_names'\n",
    "    ),\n",
    "    rules=ClassAccessor(\n",
    "        class_tag='pp', \n",
    "        class_attribute='rules'    \n",
    "    ),\n",
    "    n_iter=20\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0ac48f",
   "metadata": {},
   "source": [
    "**Note:** The arguments passed to the `pos_pred_rules` and `rules` parameters in the `RBSOptimiser` class are `ClassAccessor` objects. This object extracts the specified attribute from the given class in the pipeline. This allows users to pass attributes from earlier steps in the pipeline as parameters of later steps in the pipeline.\n",
    "\n",
    "In this example, the names of the rules that are present in the concatenated output produced by the `ParallelPipeline` are passed to the `pos_pred_rules` parameter of the `RBSOptimiser` class - this is so the `RBSOptimiser` knows which rules predict positive cases (which, one might argue, doesn't need to be specified in this example, as we only have one type of rule set. However, when you have a set of rules - some that predict positive cases and some that predict negative cases - you must specify which rules predict what case, using the `pos_pred_rules` and `neg_pred_rules` parameters).\n",
    "\n",
    "Also, the `rules` attribute created in the `ParallelPipeline` are passed to the `rules` parameter of the `RBSOptimiser` class. This is so the rules remaining after the decision engine optimisation can be easily extracted from the trained pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0044360",
   "metadata": {},
   "source": [
    "Now we can define the steps of our Linear Pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "704496ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps=[\n",
    "    ('pp', pp),\n",
    "    ('rbs_optimiser', rbs_optimiser)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e29171d",
   "metadata": {},
   "source": [
    "Finally, we can instantiate our Linear Pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0cc56f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instatiate the LinearPipeline\n",
    "lp = LinearPipeline(\n",
    "    steps=steps,\n",
    "    verbose=2 # Set to 2 to see current step being trained\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22183400",
   "metadata": {},
   "source": [
    "## Using the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f553b7",
   "metadata": {},
   "source": [
    "### `fit` method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37c398b",
   "metadata": {},
   "source": [
    "By running the `fit` method, we sequentially run the `fit_transform` methods of each step in the pipeline, except for the last step, where the `fit` method is run. \n",
    "\n",
    "**Note:** we need to pass the unprocessed data to the rule optimiser step - we can do this by feeding a dictionary to the parameter `X`, where the key of the dictionary corresponds to the step where the given dataset (value) should be passed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e9041d5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Applying `fit_transform` method for step `pp` ---\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 19.23it/s]\n",
      "--- Applying `fit` method or step `rbs_optimiser` ---\n"
     ]
    }
   ],
   "source": [
    "lp.fit(\n",
    "    X={\n",
    "        'generator': X_train,\n",
    "        'optimiser': X_train_raw,\n",
    "    }, \n",
    "    y=y_train, \n",
    "    sample_weight=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833c43da",
   "metadata": {},
   "source": [
    "#### Outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9173994d",
   "metadata": {},
   "source": [
    "The `fit` method doesn't return anything. However, you can access the attributes of the fitted classes using the `get_params` method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1796af6d",
   "metadata": {},
   "source": [
    "To see the rules that remain after the decision engine optimisation, we can extract the `rules` attribute from the `rbs_optimiser` step in the trained pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c1494ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rules = lp.get_params()['rbs_optimiser']['rules']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883ea318",
   "metadata": {},
   "source": [
    "Then access the `rule_strings` attribute to see the logic of each rule:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0b6c6d9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'RGDT_Rule_20220221_1': \"(X['Embarked_Q']==True)&(X['Embarked_S']==False)&(X['Sex_female']==True)\",\n",
       " 'CombinedRule': \"(X['Pclass']<=2)&(X['Sex']=='female')\"}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rules.rule_strings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab23e54",
   "metadata": {},
   "source": [
    "### `fit_predict` method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f2d716",
   "metadata": {},
   "source": [
    "By running the `fit_predict` method, we sequentially run the `fit_transform` methods of each step in the pipeline, except for the last step, where the `fit_predict` method is run.\n",
    "\n",
    "**Note:** we need to pass the unprocessed data to the rule optimiser step - we can do this by feeding a dictionary to the parameter `X`, where the key of the dictionary corresponds to the step where the given dataset (value) should be passed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "363b1244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Applying `fit_transform` method for step `pp` ---\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 23.22it/s]\n",
      "--- Applying `fit` method or step `rbs_optimiser` ---\n"
     ]
    }
   ],
   "source": [
    "y_pred_train = lp.fit_predict(\n",
    "    X={\n",
    "        'generator': X_train,\n",
    "        'optimiser': X_train_raw,\n",
    "    }, \n",
    "    y=y_train, \n",
    "    sample_weight=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0adb3a",
   "metadata": {},
   "source": [
    "#### Outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca32c00",
   "metadata": {},
   "source": [
    "The `fit_predict` method returns the prediction generated by class in the final step of the pipeline - in this case, the `RBSOptimiser`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b7967465",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PassengerId\n",
       "7      0\n",
       "719    0\n",
       "686    0\n",
       "74     0\n",
       "883    0\n",
       "      ..\n",
       "107    0\n",
       "271    0\n",
       "861    0\n",
       "436    1\n",
       "103    0\n",
       "Length: 596, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6de8782",
   "metadata": {},
   "source": [
    "### `predict` method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb64136f",
   "metadata": {},
   "source": [
    "By running the `predict` method, we sequentially run the `transform` methods of each step in the pipeline, except for the last step, where the `predict` method is run. Note that before using this method, you should first run either the `fit` or `fit_predict` methods:\n",
    "\n",
    "**Note:** we need to pass the unprocessed data to the rule optimiser step - we can do this by feeding a dictionary to the parameter `X`, where the key of the dictionary corresponds to the step where the given dataset (value) should be passed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "75773514",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test = lp.predict(\n",
    "    X={\n",
    "        'generator': X_test,\n",
    "        'optimiser': X_test_raw,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ce9eec",
   "metadata": {},
   "source": [
    "#### Outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3fe3abd",
   "metadata": {},
   "source": [
    "The `predict` method returns the prediction generated by class in the final step of the pipeline - in this case, the `RBSOptimiser`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "653aee49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PassengerId\n",
       "710    0\n",
       "440    0\n",
       "841    0\n",
       "721    1\n",
       "40     0\n",
       "      ..\n",
       "716    0\n",
       "526    0\n",
       "382    0\n",
       "141    0\n",
       "174    0\n",
       "Length: 295, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ecd0fe7",
   "metadata": {},
   "source": [
    "This approach is very powerful when optimising hyperparameters for the overall performance of a Rules-Based System - see the `BayesSearchCV` class in the `rule_selection` module for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b05e17",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6a5a22224d030f6805b27da964f50b3905be89918ca593f843e32c3b2a80fa84"
  },
  "kernelspec": {
   "display_name": "iguanas_dev",
   "language": "python",
   "name": "iguanas_dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
