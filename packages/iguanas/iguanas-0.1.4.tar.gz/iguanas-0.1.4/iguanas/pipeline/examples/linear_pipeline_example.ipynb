{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4314dd6f",
   "metadata": {},
   "source": [
    "# Linear Pipeline Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3a33f2",
   "metadata": {},
   "source": [
    "A Linear Pipeline is a sequence of steps that are applied sequentially to a dataset. Each step should be an instantiated class with both `fit` and `transform` methods. The final step should be an instantiated class with both `fit` and `predict` methods.\n",
    "\n",
    "The below diagram shows the high level structure of a Linear Pipeline:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709d9098",
   "metadata": {},
   "source": [
    "<center><img src=\"images/linear_pipeline_structure.png\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7f80b3",
   "metadata": {},
   "source": [
    "It is used to carry out multiple processes sequentially, using the output of the last step as the input to the next step. One such use case is when we want to:\n",
    "\n",
    "1. Generate a set of rules.\n",
    "2. Filter the rule set to remove those which are:\n",
    "    * Poorly performing.\n",
    "    * Correlated.\n",
    "3. Use the resulting rule set to optimise a decision engine.\n",
    "\n",
    "An example of this workflow is shown below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318e87b9",
   "metadata": {},
   "source": [
    "<center><img src=\"images/linear_pipeline_example.png\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb2668b",
   "metadata": {},
   "source": [
    "To create this workflow in Iguanas, each step will be added to a Linear Pipeline (whilst maintaining the order shown).\n",
    "\n",
    "**We'll see how this workflow can be generated in the following example.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be721b29",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ddc510f",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb7115b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from iguanas.rule_generation import RuleGeneratorDT\n",
    "from iguanas.rule_optimisation import BayesianOptimiser\n",
    "from iguanas.rule_selection import SimpleFilter, CorrelatedFilter\n",
    "from iguanas.metrics import FScore, Precision, JaccardSimilarity\n",
    "from iguanas.rbs import RBSOptimiser, RBSPipeline\n",
    "from iguanas.correlation_reduction import AgglomerativeClusteringReducer\n",
    "from iguanas.pipeline import LinearPipeline\n",
    "from iguanas.pipeline.class_accessor import ClassAccessor\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from category_encoders.one_hot import OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6031c6",
   "metadata": {},
   "source": [
    "## Read in data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28354c2",
   "metadata": {},
   "source": [
    "Let's read in the famous Titanic data set and split it into training and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "938ebd72",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../../examples/dummy_data/titanic.csv', index_col='PassengerId')\n",
    "target_col = 'Survived'\n",
    "cols_to_drop = ['Name', 'Ticket', 'Cabin']\n",
    "X = df.drop([target_col] + cols_to_drop, axis=1)\n",
    "y = df[target_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb3ee24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.33,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0499265",
   "metadata": {},
   "source": [
    "## Data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae50c1c2",
   "metadata": {},
   "source": [
    "Let's apply the following simple steps to process the data:\n",
    "* One hot encode categorical variables (accounting for nulls)\n",
    "* Impute numeric features with -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fac7d3e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jlaidler/venvs/iguanas_dev/lib/python3.8/site-packages/category_encoders/utils.py:21: FutureWarning: is_categorical is deprecated and will be removed in a future version.  Use is_categorical_dtype instead\n",
      "  elif pd.api.types.is_categorical(cols):\n"
     ]
    }
   ],
   "source": [
    "# OHE\n",
    "encoder = OneHotEncoder(\n",
    "    use_cat_names=True\n",
    ")\n",
    "X_train = encoder.fit_transform(X_train)\n",
    "X_test = encoder.transform(X_test)\n",
    "\n",
    "# Impute\n",
    "X_train.fillna(-1, inplace=True)\n",
    "X_test.fillna(-1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264a6921",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d043f0",
   "metadata": {},
   "source": [
    "## Set up pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90225bcf",
   "metadata": {},
   "source": [
    "To create the worflow shown at the beginning of the notebook, let's first instantiate the classes for each step. We'll optimise our decision engine to maximise the **F1 score**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6030498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimisation metric\n",
    "f1 = FScore(beta=1)\n",
    "\n",
    "# Rule generation\n",
    "generator = RuleGeneratorDT(\n",
    "    metric=f1.fit,\n",
    "    n_total_conditions=4,\n",
    "    tree_ensemble=RandomForestClassifier(\n",
    "        n_estimators=2,\n",
    "        random_state=0\n",
    "    ),\n",
    "    target_feat_corr_types='Infer'\n",
    ")\n",
    "\n",
    "# Rule filter (performance-based)\n",
    "simple_filterer = SimpleFilter(\n",
    "    threshold=0.1, # Filter out rules with an F1 score < 0.1\n",
    "    operator='>=', \n",
    "    metric=f1.fit\n",
    ")\n",
    "\n",
    "# Rule filter (correlation-based)\n",
    "js = JaccardSimilarity()\n",
    "corr_filterer = CorrelatedFilter(\n",
    "    correlation_reduction_class=AgglomerativeClusteringReducer(\n",
    "        threshold=0.9, # Filter out rules in the same cluster with a Jaccard Similarity >= 0.9 \n",
    "        strategy='top_down', \n",
    "        similarity_function=js.fit, \n",
    "        metric=f1.fit\n",
    "    )\n",
    ")\n",
    "\n",
    "# Decision engine (to be optimised)\n",
    "rbs_pipeline = RBSPipeline(\n",
    "    config=[],\n",
    "    final_decision=0\n",
    ")\n",
    "\n",
    "# Decision engine optimiser\n",
    "rbs_optimiser = RBSOptimiser(\n",
    "    pipeline=rbs_pipeline,\n",
    "    metric=f1.fit, \n",
    "    pos_pred_rules=ClassAccessor(\n",
    "        class_tag='corr_filterer', \n",
    "        class_attribute='rules_to_keep'\n",
    "    ),\n",
    "    rules=ClassAccessor(\n",
    "        class_tag='generator',\n",
    "        class_attribute='rules'\n",
    "    ),\n",
    "    n_iter=10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f915a7",
   "metadata": {},
   "source": [
    "**Note:** The arguments passed to the `pos_pred_rules` and `rules` parameters in the `RBSOptimiser` class are `ClassAccessor` objects. This object extracts the specified attribute from the given class in the pipeline. This allows users to pass attributes from earlier steps in the pipeline as parameters of later steps in the pipeline.\n",
    "\n",
    "In this example, the names of the rules that are present after the `corr_filterer` step are passed to the `pos_pred_rules` parameter of the `RBSOptimiser` class - this is so the `RBSOptimiser` knows which rules predict positive cases (which, one might argue, doesn't need to be specified in this example, as we only have one type of rule set. However, when you have a set of rules - some that predict positive cases and some that predict negative cases - you must specify which rules predict what case, using the `pos_pred_rules` and `neg_pred_rules` parameters).\n",
    "\n",
    "Also, the `rules` attribute created in the `generator` step are passed to the `rules` parameter of the `RBSOptimiser` class. This is so the rules remaining after the decision engine optimisation can be easily extracted from the trained pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1e338a",
   "metadata": {},
   "source": [
    "Now we can create the steps of our pipeline. Each step should be a tuple of two elements:\n",
    "\n",
    "1. The first element should be a string which refers to the step.\n",
    "2. The second element should be the instantiated class which runs at that step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf79e43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = [\n",
    "    ('generator', generator),\n",
    "    ('simple_filterer', simple_filterer),\n",
    "    ('corr_filterer', corr_filterer),\n",
    "    ('rbs_optimiser', rbs_optimiser)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4fc4e4",
   "metadata": {},
   "source": [
    "Finally, we can instantiate our pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a079750e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lp = LinearPipeline(\n",
    "    steps=steps,\n",
    "    verbose=1 # Set to 1 to see overall progress\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22183400",
   "metadata": {},
   "source": [
    "## Using the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f553b7",
   "metadata": {},
   "source": [
    "### `fit` method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37c398b",
   "metadata": {},
   "source": [
    "By running the `fit` method, we sequentially run the `fit_transform` methods of each step in the pipeline, except for the last step, where the `fit` method is run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9041d5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 44.24it/s]\n"
     ]
    }
   ],
   "source": [
    "lp.fit(\n",
    "    X=X_train, \n",
    "    y=y_train,\n",
    "    sample_weight=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833c43da",
   "metadata": {},
   "source": [
    "#### Outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9173994d",
   "metadata": {},
   "source": [
    "The `fit` method doesn't return anything. However, you can access the attributes of the fitted classes using the `get_params` method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff74328",
   "metadata": {},
   "source": [
    "To see the rules that remain after the decision engine optimisation, we can extract the `rules` attribute from the `rbs_optimiser` step in the trained pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79ad94ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "rules = lp.get_params()['rbs_optimiser']['rules']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4503b249",
   "metadata": {},
   "source": [
    "Then access the `rule_strings` attribute to see the logic of each rule:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a40ec110",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'RGDT_Rule_20220221_0': \"(X['Age']>14.5)&(X['Pclass']<=2)&(X['Sex_female']==True)\",\n",
       " 'RGDT_Rule_20220221_5': \"(X['Embarked_C']==True)&(X['Pclass']<=1)\",\n",
       " 'RGDT_Rule_20220221_8': \"(X['Embarked_S']==False)&(X['Fare']>14.85)&(X['Sex_female']==True)\",\n",
       " 'RGDT_Rule_20220221_9': \"(X['Embarked_S']==False)&(X['Fare']>14.9771)&(X['Sex_female']==True)\",\n",
       " 'RGDT_Rule_20220221_13': \"(X['Fare']>52.2771)\",\n",
       " 'RGDT_Rule_20220221_16': \"(X['Pclass']<=1)&(X['Sex_male']==False)\",\n",
       " 'RGDT_Rule_20220221_17': \"(X['Pclass']<=2)&(X['Sex_female']==True)\",\n",
       " 'RGDT_Rule_20220221_18': \"(X['Pclass']<=2)&(X['Sex_male']==False)\",\n",
       " 'RGDT_Rule_20220221_19': \"(X['Sex_female']==True)\",\n",
       " 'RGDT_Rule_20220221_20': \"(X['Sex_male']==False)\",\n",
       " 'RGDT_Rule_20220221_21': \"(X['SibSp']>=1)\"}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rules.rule_strings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab23e54",
   "metadata": {},
   "source": [
    "### `fit_predict` method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f2d716",
   "metadata": {},
   "source": [
    "By running the `fit_predict` method, we sequentially run the `fit_transform` methods of each step in the pipeline, except for the last step, where the `fit_predict` method is run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "363b1244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 63.26it/s]\n"
     ]
    }
   ],
   "source": [
    "y_pred_train = lp.fit_predict(\n",
    "    X=X_train, \n",
    "    y=y_train,\n",
    "    sample_weight=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0adb3a",
   "metadata": {},
   "source": [
    "#### Outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca32c00",
   "metadata": {},
   "source": [
    "The `fit_predict` method returns the prediction generated by class in the final step of the pipeline - in this case, the `RBSOptimiser`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b7967465",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PassengerId\n",
       "7      0\n",
       "719    0\n",
       "686    1\n",
       "74     1\n",
       "883    1\n",
       "      ..\n",
       "107    1\n",
       "271    0\n",
       "861    1\n",
       "436    1\n",
       "103    1\n",
       "Length: 596, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6de8782",
   "metadata": {},
   "source": [
    "### `predict` method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb64136f",
   "metadata": {},
   "source": [
    "By running the `predict` method, we sequentially run the `transform` methods of each step in the pipeline, except for the last step, where the `predict` method is run. Note that before using this method, you should first run either the `fit` or `fit_predict` methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "75773514",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test = lp.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ce9eec",
   "metadata": {},
   "source": [
    "#### Outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3fe3abd",
   "metadata": {},
   "source": [
    "The `predict` method returns the prediction generated by class in the final step of the pipeline - in this case, the `RBSOptimiser`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "653aee49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PassengerId\n",
       "710    1\n",
       "440    0\n",
       "841    0\n",
       "721    1\n",
       "40     1\n",
       "      ..\n",
       "716    0\n",
       "526    0\n",
       "382    1\n",
       "141    1\n",
       "174    0\n",
       "Length: 295, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6147013a",
   "metadata": {},
   "source": [
    "We can now calculate the F1 score of our pipeline using the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c2eaed0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7038327526132405"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1.fit(y_pred_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ecd0fe7",
   "metadata": {},
   "source": [
    "This approach is very powerful when optimising hyperparameters for the overall performance of a Rules-Based System - see the `BayesSearchCV` class in the `rule_selection` module for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b05e17",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8c72ae",
   "metadata": {},
   "source": [
    "## Using the initial data in a subsequent pipeline step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3dcd8e6",
   "metadata": {},
   "source": [
    "The example above assumes that each subsequent step is expecting the input to be the output from the former step. However, there are cases where you may want a subsequent step in a pipeline to use an attribute created from a former step, but the input to be the initial data. \n",
    "\n",
    "For example, consider the below workflow:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b306b769",
   "metadata": {},
   "source": [
    "![title](images/linear_pipeline_use_init_data_incorrect.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bca89e0",
   "metadata": {},
   "source": [
    "This workflow would fail as part of a standard Linear Pipeline, since the rule optimisation step requires the initial dataset to optimise the rules - in this workflow, the output of the rule generation step (which is a dataset of the binary columns of the rules) would be passed to the `X` parameter of the rule optimiser's `fit_transform` method, which would fail as the features in the rules would not be present in that dataset.\n",
    "\n",
    "Instead, we need to construct a workflow like so:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9892644",
   "metadata": {},
   "source": [
    "![title](images/linear_pipeline_use_init_data.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1bbe09a",
   "metadata": {},
   "source": [
    "Here, we generate a rule set using the the processed data and the binary target. We then pass this rule set to the rule optimiser, but use the initial data as the input.\n",
    "\n",
    "We can create this workflow using the `use_init_data` parameter in the `LinearPipeline` class - let's see how this can be done below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "045c10f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimisation metric\n",
    "f1 = FScore(beta=1)\n",
    "\n",
    "# Rule generation\n",
    "generator = RuleGeneratorDT(\n",
    "    metric=f1.fit,\n",
    "    n_total_conditions=4,\n",
    "    tree_ensemble=RandomForestClassifier(\n",
    "        n_estimators=2,\n",
    "        random_state=0\n",
    "    ),\n",
    "    target_feat_corr_types='Infer'\n",
    ")\n",
    "\n",
    "# Rule optimisation\n",
    "optimiser = BayesianOptimiser(\n",
    "    rule_lambdas=ClassAccessor( # Use a ClassAccessor to extract rule_lambdas from the generator step\n",
    "        class_tag='generator',\n",
    "        class_attribute='rule_lambdas'\n",
    "    ),\n",
    "    lambda_kwargs=ClassAccessor( # Use a ClassAccessor to extract lambda_kwargs from the generator step\n",
    "        class_tag='generator',\n",
    "        class_attribute='lambda_kwargs'\n",
    "    ),\n",
    "    metric=f1.fit,\n",
    "    n_iter=10\n",
    ")\n",
    "\n",
    "# Decision engine (to be optimised)\n",
    "rbs_pipeline = RBSPipeline(\n",
    "    config=[],\n",
    "    final_decision=0\n",
    ")\n",
    "\n",
    "# Decision engine optimiser\n",
    "rbs_optimiser = RBSOptimiser(\n",
    "    pipeline=rbs_pipeline,\n",
    "    metric=f1.fit, \n",
    "    pos_pred_rules=ClassAccessor(\n",
    "        class_tag='optimiser', \n",
    "        class_attribute='rule_names'\n",
    "    ),\n",
    "    rules=ClassAccessor(\n",
    "        class_tag='optimiser',\n",
    "        class_attribute='rules'\n",
    "    ),\n",
    "    n_iter=10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6862fb6b",
   "metadata": {},
   "source": [
    "Now, when we set up our `LinearPipeline`, we need to specify that the `optimiser` step will use the initial dataset. We do this by adding the pipeline step tag to a list and passing that to the `use_init_data` parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "40f6a057",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = [\n",
    "    ('generator', generator),\n",
    "    ('optimiser', optimiser),\n",
    "    ('rbs_optimiser', rbs_optimiser)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ba61f101",
   "metadata": {},
   "outputs": [],
   "source": [
    "lp = LinearPipeline(\n",
    "    steps=steps,\n",
    "    use_init_data=['optimiser'],\n",
    "    verbose=2 # Set to 2 to see current step being trained\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50dce1e7",
   "metadata": {},
   "source": [
    "Now we can use the `LinearPipeline` methods outlined earlier in the notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aa704f94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Applying `fit_transform` method for step `generator` ---\n",
      "--- Applying `fit_transform` method for step `optimiser` ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jlaidler/Documents/Iguanas/iguanas/rule_optimisation/_base_optimiser.py:218: UserWarning: Rules `RGDT_Rule_20220221_6`, `RGDT_Rule_20220221_7`, `RGDT_Rule_20220221_12`, `RGDT_Rule_20220221_19`, `RGDT_Rule_20220221_20` have no optimisable conditions - unable to optimise these rules\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Applying `fit` method or step `rbs_optimiser` ---\n"
     ]
    }
   ],
   "source": [
    "# fit method\n",
    "lp.fit(\n",
    "    X=X_train,\n",
    "    y=y_train,\n",
    "    sample_weight=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ee8ed10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rules = lp.get_params()['rbs_optimiser']['rules']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2e2457e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'RGDT_Rule_20220221_1': \"(X['Age']>10.941004768044909)&(X['Sex_female']==True)\",\n",
       " 'RGDT_Rule_20220221_2': \"(X['Age']>47)&(X['Embarked_S']==False)&(X['Fare']>52.2771)\",\n",
       " 'RGDT_Rule_20220221_4': \"(X['Age']>10.941004768044909)\",\n",
       " 'RGDT_Rule_20220221_5': \"(X['Embarked_C']==True)&(X['Pclass']<=2)\",\n",
       " 'RGDT_Rule_20220221_8': \"(X['Embarked_S']==False)&(X['Fare']>14.85)&(X['Sex_female']==True)\",\n",
       " 'RGDT_Rule_20220221_9': \"(X['Embarked_S']==False)&(X['Fare']>14.9771)&(X['Sex_female']==True)\",\n",
       " 'RGDT_Rule_20220221_10': \"(X['Embarked_S']==False)&(X['Fare']>47.162206755851706)\",\n",
       " 'RGDT_Rule_20220221_15': \"(X['Pclass']<=2)\",\n",
       " 'RGDT_Rule_20220221_16': \"(X['Pclass']<=2)&(X['Sex_male']==False)\",\n",
       " 'RGDT_Rule_20220221_17': \"(X['Pclass']<=2)&(X['Sex_female']==True)\",\n",
       " 'RGDT_Rule_20220221_18': \"(X['Pclass']<=2)&(X['Sex_male']==False)\"}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rules.rule_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "41becab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Applying `fit_transform` method for step `generator` ---\n",
      "--- Applying `fit_transform` method for step `optimiser` ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jlaidler/Documents/Iguanas/iguanas/rule_optimisation/_base_optimiser.py:218: UserWarning: Rules `RGDT_Rule_20220221_6`, `RGDT_Rule_20220221_7`, `RGDT_Rule_20220221_12`, `RGDT_Rule_20220221_19`, `RGDT_Rule_20220221_20` have no optimisable conditions - unable to optimise these rules\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Applying `fit` method or step `rbs_optimiser` ---\n"
     ]
    }
   ],
   "source": [
    "# fit_predict method\n",
    "y_pred_train = lp.fit_predict(\n",
    "    X=X_train, \n",
    "    y=y_train,\n",
    "    sample_weight=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1c7d2b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict method\n",
    "y_pred_test = lp.predict(X=X_test)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6a5a22224d030f6805b27da964f50b3905be89918ca593f843e32c3b2a80fa84"
  },
  "kernelspec": {
   "display_name": "iguanas_dev",
   "language": "python",
   "name": "iguanas_dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
